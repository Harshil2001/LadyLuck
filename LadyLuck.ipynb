{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jG7ZEc_982io"
      },
      "source": [
        "# StyleGAN2-ADA-PyTorch\n",
        "\n",
        "**Notes**\n",
        "* Training and Inference sections should be fairly stable. I’ll slowly add new features but it should work for most mainstream use cases.\n",
        "* Advanced Features are being documented toward the bottom of this notebook\n",
        "\n",
        "---\n",
        "\n",
        "If you find this notebook useful, consider signing up for my [Patreon](https://www.patreon.com/bustbright) or [YouTube channel](https://www.youtube.com/channel/UCaZuPdmZ380SFUMKHVsv_AA/join). You can also send me a one-time payment on [Venmo](https://venmo.com/Derrick-Schultz)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj4PG4_i9Alt"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGEXPcFJ9UTY"
      },
      "source": [
        "Let’s start by checking to see what GPU we’ve been assigned. Ideally we get a V100, but a P100 is fine too. Other GPUs may lead to issues."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7VVICTCvd4mc",
        "outputId": "3a88796d-dc79-4ddd-f657-debd42568810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-86125ce7-5489-877c-8df1-254f934fc26c)\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSV_HEoD9dxo"
      },
      "source": [
        "Next let’s connect our Google Drive account. This is optional but highly recommended."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuVPuJmbigRs",
        "outputId": "337f9c16-06a0-4fa9-8e0e-3b6a2b8831dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTjVmfSK9CYa"
      },
      "source": [
        "## Install repo\n",
        "\n",
        "The next cell will install the StyleGAN repository in Google Drive. If you have already installed it it will just move into that folder. If you don’t have Google Drive connected it will just install the necessary code in Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B8ADVNpBh8Ox",
        "outputId": "0c99945a-13e7-4a33-88b8-d57ac284e9d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Collecting ninja\n",
            "  Downloading ninja-1.10.2.3-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (108 kB)\n",
            "\u001b[K     |████████████████████████████████| 108 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting opensimplex\n",
            "  Downloading opensimplex-0.3-py3-none-any.whl (15 kB)\n",
            "Collecting torch==1.7.1\n",
            "  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 776.8 MB 18 kB/s \n",
            "\u001b[?25hCollecting torchvision==0.8.2\n",
            "  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 13.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.7.1) (1.19.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.8.2) (7.1.2)\n",
            "Installing collected packages: torch, torchvision, opensimplex, ninja\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.10.0+cu111\n",
            "    Uninstalling torch-1.10.0+cu111:\n",
            "      Successfully uninstalled torch-1.10.0+cu111\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.11.1+cu111\n",
            "    Uninstalling torchvision-0.11.1+cu111:\n",
            "      Successfully uninstalled torchvision-0.11.1+cu111\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchtext 0.11.0 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\n",
            "torchaudio 0.10.0+cu111 requires torch==1.10.0, but you have torch 1.7.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ninja-1.10.2.3 opensimplex-0.3 torch-1.7.1 torchvision-0.8.2\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "if os.path.isdir(\"/content/drive/MyDrive/colab-sg2-ada-pytorch\"):\n",
        "    %cd \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "elif os.path.isdir(\"/content/drive/\"):\n",
        "    #install script\n",
        "    %cd \"/content/drive/MyDrive/\"\n",
        "    !mkdir colab-sg2-ada-pytorch\n",
        "    %cd colab-sg2-ada-pytorch\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU -O /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/pretrained/wikiart.pkl\n",
        "else:\n",
        "    !git clone https://github.com/dvschultz/stylegan2-ada-pytorch\n",
        "    %cd stylegan2-ada-pytorch\n",
        "    !mkdir downloads\n",
        "    !mkdir datasets\n",
        "    !mkdir pretrained\n",
        "    %cd pretrained\n",
        "    !gdown --id 1-5xZkD8ajXw1DdopTkH_rAoCsD72LhKU\n",
        "    %cd ../\n",
        "\n",
        "!pip install ninja opensimplex torch==1.7.1 torchvision==0.8.2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jMmUpn4DWRe"
      },
      "source": [
        "You probably don’t need to run this, but this will update your repo to the latest and greatest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uV9bdvzeDRPd",
        "outputId": "51cd24e9-463f-468c-812f-7f03cfc960b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\n",
            "Already up to date.\n",
            "Saved working directory and index state WIP on main: 464100c lil cleanup\n"
          ]
        }
      ],
      "source": [
        "%cd \"/content/drive/My Drive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch\"\n",
        "!git config --global user.name \"test\"\n",
        "!git config --global user.email \"test@test.com\"\n",
        "!git fetch origin\n",
        "!git pull\n",
        "!git stash\n",
        "!git checkout origin/main -- train.py generate.py legacy.py closed_form_factorization.py flesh_digression.py apply_factor.py README.md calc_metrics.py training/stylegan2_multi.py training/training_loop.py util/utilgan.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZkcJ58P97Ls"
      },
      "source": [
        "## Dataset Preparation\n",
        "\n",
        "Upload a .zip of square images to the `datasets` folder. Previously you had to convert your model to .tfrecords. That’s no longer needed :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5B-h6FpB9FaK"
      },
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNc-3wTO-MUd"
      },
      "source": [
        "Below are a series of variables you need to set to run the training. You probably won’t need to touch most of them.\n",
        "\n",
        "* `dataset_path`: this is the path to your .zip file\n",
        "* `resume_from`: if you’re starting a new dataset I recommend `'ffhq1024'` or `'./pretrained/wikiart.pkl'`\n",
        "* `mirror_x` and `mirror_y`: Allow the dataset to use horizontal or vertical mirroring."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JV0W6yxP-UIn"
      },
      "outputs": [],
      "source": [
        "#required: definitely edit these!\n",
        "dataset_path = '/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/ip\\ dataset.zip'\n",
        "#resume_from = './pretrained/wikiart.pkl'\n",
        "resume_from = './results/00026-ip\\ dataset-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000135.pkl'\n",
        "aug_strength = 0.191\n",
        "train_count = 114\n",
        "mirror_x = True\n",
        "#mirror_y = False\n",
        "\n",
        "#optional: you might not need to edit these\n",
        "gamma_value = 50.0\n",
        "augs = 'bg'\n",
        "config = '11gb-gpu'\n",
        "snapshot_count = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "EL-M7WnnfMDI",
        "outputId": "5fa3209b-d5cb-4cdf-ec7a-4e32beb52479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training options:\n",
            "{\n",
            "  \"num_gpus\": 1,\n",
            "  \"image_snapshot_ticks\": 1,\n",
            "  \"network_snapshot_ticks\": 1,\n",
            "  \"metrics\": [],\n",
            "  \"random_seed\": 0,\n",
            "  \"training_set_kwargs\": {\n",
            "    \"class_name\": \"training.dataset.ImageFolderDataset\",\n",
            "    \"path\": \"/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/ip dataset.zip\",\n",
            "    \"use_labels\": false,\n",
            "    \"max_size\": 2689,\n",
            "    \"xflip\": true,\n",
            "    \"resolution\": 256\n",
            "  },\n",
            "  \"data_loader_kwargs\": {\n",
            "    \"pin_memory\": true,\n",
            "    \"num_workers\": 3,\n",
            "    \"prefetch_factor\": 2\n",
            "  },\n",
            "  \"G_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Generator\",\n",
            "    \"z_dim\": 512,\n",
            "    \"w_dim\": 512,\n",
            "    \"mapping_kwargs\": {\n",
            "      \"num_layers\": 8\n",
            "    },\n",
            "    \"synthesis_kwargs\": {\n",
            "      \"channel_base\": 32768,\n",
            "      \"channel_max\": 512,\n",
            "      \"num_fp16_res\": 4,\n",
            "      \"conv_clamp\": 256\n",
            "    }\n",
            "  },\n",
            "  \"D_kwargs\": {\n",
            "    \"class_name\": \"training.networks.Discriminator\",\n",
            "    \"block_kwargs\": {},\n",
            "    \"mapping_kwargs\": {},\n",
            "    \"epilogue_kwargs\": {\n",
            "      \"mbstd_group_size\": 4\n",
            "    },\n",
            "    \"channel_base\": 32768,\n",
            "    \"channel_max\": 512,\n",
            "    \"num_fp16_res\": 4,\n",
            "    \"conv_clamp\": 256\n",
            "  },\n",
            "  \"G_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"D_opt_kwargs\": {\n",
            "    \"class_name\": \"torch.optim.Adam\",\n",
            "    \"lr\": 0.002,\n",
            "    \"betas\": [\n",
            "      0,\n",
            "      0.99\n",
            "    ],\n",
            "    \"eps\": 1e-08\n",
            "  },\n",
            "  \"loss_kwargs\": {\n",
            "    \"class_name\": \"training.loss.StyleGAN2Loss\",\n",
            "    \"r1_gamma\": 50.0\n",
            "  },\n",
            "  \"total_kimg\": 25000,\n",
            "  \"batch_size\": 4,\n",
            "  \"batch_gpu\": 4,\n",
            "  \"ema_kimg\": 10,\n",
            "  \"ema_rampup\": null,\n",
            "  \"nimg\": 114000,\n",
            "  \"ada_target\": 0.6,\n",
            "  \"augment_p\": 0.191,\n",
            "  \"augment_kwargs\": {\n",
            "    \"class_name\": \"training.augment.AugmentPipe\",\n",
            "    \"xflip\": 1,\n",
            "    \"rotate90\": 1,\n",
            "    \"xint\": 1,\n",
            "    \"scale\": 1,\n",
            "    \"rotate\": 1,\n",
            "    \"aniso\": 1,\n",
            "    \"xfrac\": 1\n",
            "  },\n",
            "  \"resume_pkl\": \"./results/00026-ip dataset-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000135.pkl\",\n",
            "  \"ada_kimg\": 100,\n",
            "  \"run_dir\": \"./results/00027-ip dataset-mirror-11gb-gpu-gamma50-bg-resumecustom\"\n",
            "}\n",
            "\n",
            "Output directory:   ./results/00027-ip dataset-mirror-11gb-gpu-gamma50-bg-resumecustom\n",
            "Training data:      /content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/datasets/ip dataset.zip\n",
            "Training duration:  25000 kimg\n",
            "Number of GPUs:     1\n",
            "Number of images:   2689\n",
            "Image resolution:   256\n",
            "Conditional model:  False\n",
            "Dataset x-flips:    True\n",
            "\n",
            "Creating output directory...\n",
            "Launching processes...\n",
            "Loading training set...\n",
            "\n",
            "Num images:  5378\n",
            "Image shape: [3, 256, 256]\n",
            "Label shape: [0]\n",
            "\n",
            "Constructing networks...\n",
            "starting G epochs:  0.456\n",
            "Resuming from \"./results/00026-ip dataset-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000135.pkl\"\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
            "\n",
            "Generator             Parameters  Buffers  Output shape        Datatype\n",
            "---                   ---         ---      ---                 ---     \n",
            "mapping.fc0           262656      -        [4, 512]            float32 \n",
            "mapping.fc1           262656      -        [4, 512]            float32 \n",
            "mapping.fc2           262656      -        [4, 512]            float32 \n",
            "mapping.fc3           262656      -        [4, 512]            float32 \n",
            "mapping.fc4           262656      -        [4, 512]            float32 \n",
            "mapping.fc5           262656      -        [4, 512]            float32 \n",
            "mapping.fc6           262656      -        [4, 512]            float32 \n",
            "mapping.fc7           262656      -        [4, 512]            float32 \n",
            "mapping               -           512      [4, 14, 512]        float32 \n",
            "synthesis.b4.conv1    2622465     32       [4, 512, 4, 4]      float32 \n",
            "synthesis.b4.torgb    264195      -        [4, 3, 4, 4]        float32 \n",
            "synthesis.b4:0        8192        16       [4, 512, 4, 4]      float32 \n",
            "synthesis.b4:1        -           -        [4, 512, 4, 4]      float32 \n",
            "synthesis.b8.conv0    2622465     80       [4, 512, 8, 8]      float32 \n",
            "synthesis.b8.conv1    2622465     80       [4, 512, 8, 8]      float32 \n",
            "synthesis.b8.torgb    264195      -        [4, 3, 8, 8]        float32 \n",
            "synthesis.b8:0        -           16       [4, 512, 8, 8]      float32 \n",
            "synthesis.b8:1        -           -        [4, 512, 8, 8]      float32 \n",
            "synthesis.b16.conv0   2622465     272      [4, 512, 16, 16]    float32 \n",
            "synthesis.b16.conv1   2622465     272      [4, 512, 16, 16]    float32 \n",
            "synthesis.b16.torgb   264195      -        [4, 3, 16, 16]      float32 \n",
            "synthesis.b16:0       -           16       [4, 512, 16, 16]    float32 \n",
            "synthesis.b16:1       -           -        [4, 512, 16, 16]    float32 \n",
            "synthesis.b32.conv0   2622465     1040     [4, 512, 32, 32]    float16 \n",
            "synthesis.b32.conv1   2622465     1040     [4, 512, 32, 32]    float16 \n",
            "synthesis.b32.torgb   264195      -        [4, 3, 32, 32]      float16 \n",
            "synthesis.b32:0       -           16       [4, 512, 32, 32]    float16 \n",
            "synthesis.b32:1       -           -        [4, 512, 32, 32]    float32 \n",
            "synthesis.b64.conv0   2622465     4112     [4, 512, 64, 64]    float16 \n",
            "synthesis.b64.conv1   2622465     4112     [4, 512, 64, 64]    float16 \n",
            "synthesis.b64.torgb   264195      -        [4, 3, 64, 64]      float16 \n",
            "synthesis.b64:0       -           16       [4, 512, 64, 64]    float16 \n",
            "synthesis.b64:1       -           -        [4, 512, 64, 64]    float32 \n",
            "synthesis.b128.conv0  1442561     16400    [4, 256, 128, 128]  float16 \n",
            "synthesis.b128.conv1  721409      16400    [4, 256, 128, 128]  float16 \n",
            "synthesis.b128.torgb  132099      -        [4, 3, 128, 128]    float16 \n",
            "synthesis.b128:0      -           16       [4, 256, 128, 128]  float16 \n",
            "synthesis.b128:1      -           -        [4, 256, 128, 128]  float32 \n",
            "synthesis.b256.conv0  426369      65552    [4, 128, 256, 256]  float16 \n",
            "synthesis.b256.conv1  213249      65552    [4, 128, 256, 256]  float16 \n",
            "synthesis.b256.torgb  66051       -        [4, 3, 256, 256]    float16 \n",
            "synthesis.b256:0      -           16       [4, 128, 256, 256]  float16 \n",
            "synthesis.b256:1      -           -        [4, 128, 256, 256]  float32 \n",
            "---                   ---         ---      ---                 ---     \n",
            "Total                 30034338    175568   -                   -       \n",
            "\n",
            "\n",
            "Discriminator  Parameters  Buffers  Output shape        Datatype\n",
            "---            ---         ---      ---                 ---     \n",
            "b256.fromrgb   512         16       [4, 128, 256, 256]  float16 \n",
            "b256.skip      32768       16       [4, 256, 128, 128]  float16 \n",
            "b256.conv0     147584      16       [4, 128, 256, 256]  float16 \n",
            "b256.conv1     295168      16       [4, 256, 128, 128]  float16 \n",
            "b256           -           16       [4, 256, 128, 128]  float16 \n",
            "b128.skip      131072      16       [4, 512, 64, 64]    float16 \n",
            "b128.conv0     590080      16       [4, 256, 128, 128]  float16 \n",
            "b128.conv1     1180160     16       [4, 512, 64, 64]    float16 \n",
            "b128           -           16       [4, 512, 64, 64]    float16 \n",
            "b64.skip       262144      16       [4, 512, 32, 32]    float16 \n",
            "b64.conv0      2359808     16       [4, 512, 64, 64]    float16 \n",
            "b64.conv1      2359808     16       [4, 512, 32, 32]    float16 \n",
            "b64            -           16       [4, 512, 32, 32]    float16 \n",
            "b32.skip       262144      16       [4, 512, 16, 16]    float16 \n",
            "b32.conv0      2359808     16       [4, 512, 32, 32]    float16 \n",
            "b32.conv1      2359808     16       [4, 512, 16, 16]    float16 \n",
            "b32            -           16       [4, 512, 16, 16]    float16 \n",
            "b16.skip       262144      16       [4, 512, 8, 8]      float32 \n",
            "b16.conv0      2359808     16       [4, 512, 16, 16]    float32 \n",
            "b16.conv1      2359808     16       [4, 512, 8, 8]      float32 \n",
            "b16            -           16       [4, 512, 8, 8]      float32 \n",
            "b8.skip        262144      16       [4, 512, 4, 4]      float32 \n",
            "b8.conv0       2359808     16       [4, 512, 8, 8]      float32 \n",
            "b8.conv1       2359808     16       [4, 512, 4, 4]      float32 \n",
            "b8             -           16       [4, 512, 4, 4]      float32 \n",
            "b4.mbstd       -           -        [4, 513, 4, 4]      float32 \n",
            "b4.conv        2364416     16       [4, 512, 4, 4]      float32 \n",
            "b4.fc          4194816     -        [4, 512]            float32 \n",
            "b4.out         513         -        [4, 1]              float32 \n",
            "---            ---         ---      ---                 ---     \n",
            "Total          28864129    416      -                   -       \n",
            "\n",
            "Setting up augmentation...\n",
            "Distributing across 1 GPUs...\n",
            "Setting up training phases...\n",
            "Exporting sample images...\n",
            "Initializing logs...\n",
            "Training for 25000 kimg...\n",
            "\n",
            "tick 0     kimg 114.0    time 2m 36s       sec/tick 21.0    sec/kimg 5243.11 maintenance 135.0  cpumem 4.18   gpumem 8.98   augment 0.191\n",
            "tick 1     kimg 118.0    time 55m 51s      sec/tick 3155.8  sec/kimg 788.96  maintenance 39.1   cpumem 4.43   gpumem 2.68   augment 0.208\n",
            "tick 2     kimg 122.0    time 1h 49m 00s   sec/tick 3150.0  sec/kimg 787.49  maintenance 39.0   cpumem 4.69   gpumem 2.68   augment 0.225\n",
            "tick 3     kimg 126.0    time 2h 42m 07s   sec/tick 3147.6  sec/kimg 786.91  maintenance 39.1   cpumem 4.64   gpumem 2.68   augment 0.229\n",
            "tick 4     kimg 130.0    time 3h 35m 27s   sec/tick 3161.8  sec/kimg 790.46  maintenance 39.0   cpumem 4.24   gpumem 2.69   augment 0.225\n"
          ]
        }
      ],
      "source": [
        "!python train.py --gpus=1 --cfg=$config --metrics=None --outdir=./results --data=$dataset_path --snap=$snapshot_count --resume=$resume_from --augpipe=$augs --initstrength=$aug_strength --gamma=$gamma_value --mirror=$mirror_x --mirrory=False --nkimg=$train_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VznRirOE5ENI"
      },
      "source": [
        "## Convert Legacy Model\n",
        "\n",
        "If you have an older version of a model (Tensorflow based StyleGAN, or Runway downloaded .pkl file) you’ll need to convert to the newest version. If you’ve trained in this notebook you do **not** need to use this cell.\n",
        "\n",
        "`--source`: path to model that you want to convert\n",
        "\n",
        "`--dest`: path and file name to convert to."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzkP-Rww5Np9"
      },
      "outputs": [],
      "source": [
        "!python legacy.py --source=/content/drive/MyDrive/runway.pkl --dest=/content/drive/MyDrive/colab-sg2-ada-pytorch/stylegan2-ada-pytorch/runway.pkl"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6EtrPqL9ILk"
      },
      "source": [
        "## Testing/Inference\n",
        "\n",
        "Also known as \"Inference\", \"Evaluation\" or \"Testing\" the model. This is the process of usinng your trained model to generate new material, usually images or videos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYdyfH0O8In_"
      },
      "source": [
        "### Generate Single Images\n",
        "\n",
        "`--network`: Make sure the `--network` argument points to your .pkl file. (My preferred method is to right click on the file in the Files pane to your left and choose `Copy Path`, then paste that into the argument after the `=` sign).\n",
        "\n",
        "`--seeds`: This allows you to choose random seeds from the model. Remember that our input to StyleGAN is a 512-dimensional array. These seeds will generate those 512 values. Each seed will generate a different, random array. The same seed value will also always generate the same random array, so we can later use it for other purposes like interpolation.\n",
        "\n",
        "`--truncation`: Truncation, well, truncates the latent space. This can have a subtle or dramatic affect on your images depending on the value you use. The smaller the number the more realistic your images should appear, but this will also affect diversity. Most people choose between 0.5 and 1.0, but technically it's infinite.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VYRXenMoZSHf",
        "outputId": "b1a359ca-2393-4e70-8084-1196685855f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading networks from \"./results/00017-ip dataset-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000061.pkl\"...\n",
            "Generating image for seed 10000009 (0/1) ...\n",
            "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
            "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n"
          ]
        }
      ],
      "source": [
        "#!python generate.py --outdir=/content/out/images/ --trunc=0.8 --seeds=0 --network=/content/drive/MyDrive/ladiescrop-network-snapshot-012885.pkl\n",
        "!python generate.py --outdir=/content/drive/MyDrive/Genn --trunc=0.8 --seeds=10000009 --network=./results/00017-ip\\ dataset-mirror-11gb-gpu-gamma50-bg-resumecustom/network-snapshot-000061.pkl"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}